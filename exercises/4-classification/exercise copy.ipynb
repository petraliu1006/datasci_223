{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on `emnist`\n",
    "\n",
    "## 1. Create `Readme.md` to document your work\n",
    "\n",
    "Explain your choices, process, and outcomes.\n",
    "\n",
    "## 2. Classify all symbols\n",
    "\n",
    "### Choose a model\n",
    "\n",
    "Your choice of model! Choose wisely...\n",
    "\n",
    "### Train away!\n",
    "\n",
    "Is do you need to tune any parameters? Is the model expecting data in a different format?\n",
    "\n",
    "### Evaluate the model\n",
    "\n",
    "Evaluate the models on the test set, analyze the confusion matrix to see where the model performs well and where it struggles.\n",
    "\n",
    "### Investigate subsets\n",
    "\n",
    "On which classes does the model perform well? Poorly? Evaluate again, excluding easily confused symbols (such as 'O' and '0').\n",
    "\n",
    "### Improve performance\n",
    "\n",
    "Brainstorm for improving the performance. This could include trying different architectures, adding more layers, changing the loss function, or using data augmentation techniques.\n",
    "\n",
    "## 2. Classify digits vs. letters model showdown\n",
    "\n",
    "Perform a full showdown classifying digits vs letters:\n",
    "\n",
    "1. Create a column for whether each row is a digit or a letter\n",
    "2. Choose an evaluation metric \n",
    "3. Choose several candidate models to train\n",
    "4. Divide data to reserve a validation set that will NOT be used in training/testing\n",
    "5. K-fold train/test\n",
    "    1. Create train/test splits from the non-validation dataset \n",
    "    2. Train each candidate model (best practice: use the same split for all models)\n",
    "    3. Apply the model the the test split \n",
    "    4. (*Optional*) Perform hyper-parametric search\n",
    "    5. Record the model evaluation metrics\n",
    "    6. Repeat with a new train/test split\n",
    "6. Promote winner, apply model to validation set\n",
    "7. (*Optional*) Perform hyper-parametric search, if applicable\n",
    "8. Report model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_classes = 62\n",
    "image_size = 28  \n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root= './data',train=True,transform=transforms.ToTensor(),download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',train=False,transform=transforms.ToTensor(),download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(12):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(train_dataset.train_data[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"number/symbol: {}\".format(train_dataset.train_labels[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 10, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(10, 20, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 50),\n",
    "            torch.nn.Linear(50, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.conv1(x)  \n",
    "        x = self.conv2(x)  \n",
    "        x = x.view(batch_size, -1)  \n",
    "        x = self.fc(x)\n",
    "        return x  \n",
    "\n",
    "model = Net()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)  \n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    running_total = 0\n",
    "    running_correct = 0\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        inputs, target = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        running_total += inputs.shape[0]\n",
    "        running_correct += (predicted == target).sum().item()\n",
    "\n",
    "        if batch_idx % 300 == 299:  \n",
    "            print('[%d, %5d]: loss: %.3f , acc: %.2f %%'\n",
    "                  % (epoch + 1, batch_idx + 1, running_loss / 300, 100 * running_correct / running_total))\n",
    "            running_loss = 0.0  \n",
    "            running_total = 0\n",
    "            running_correct = 0  \n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  \n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  \n",
    "            total += labels.size(0)  \n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = correct / total\n",
    "    print('[%d / %d]: Accuracy on test set: %.1f %% ' % (epoch+1, EPOCH, 100 * acc))  \n",
    "    return acc\n",
    "\n",
    "batch_size = 16 # 64\n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "EPOCH = 10\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    acc_list_test = []\n",
    "    for epoch in range(EPOCH):\n",
    "        train(epoch)\n",
    "        # if epoch % 10 == 9: \n",
    "        acc_test = test()\n",
    "        acc_list_test.append(acc_test)\n",
    "\n",
    "    plt.plot(acc_list_test)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy On TestSet')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
