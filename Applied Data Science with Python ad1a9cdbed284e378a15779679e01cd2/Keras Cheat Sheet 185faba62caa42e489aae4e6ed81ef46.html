<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Keras Cheat Sheet</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="185faba6-2caa-42e4-89aa-e4e6ed81ef46" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">üê¥</span></div><h1 class="page-title">Keras Cheat Sheet</h1><p class="page-description"></p></header><div class="page-body"><h1 id="83d21bd6-e407-44b4-8c09-4e7de10241f0" class="">References for learning Keras</h1><ul id="7ae002e4-8424-4b7d-bcaf-6099459f54c1" class="bulleted-list"><li style="list-style-type:disc"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781098125967/"><em>Hands-on Machine Learning</em></a>: (G√©ron) and companion <a href="https://github.com/ageron/handson-ml3">repository</a></li></ul><ul id="7d0f942e-f480-4844-b111-2e903e2c3883" class="bulleted-list"><li style="list-style-type:disc"><a href="https://www.tensorflow.org/tutorials"><em>TensorFlow Tutorials</em></a><strong>:</strong> Official tutorials covering various aspects of TensorFlow, from basics to advanced techniques</li></ul><ul id="443f83e9-9abc-4663-bf1d-3fc7daa80ee9" class="bulleted-list"><li style="list-style-type:disc"><a href="https://keras.io/"><em>Keras Documentation</em></a><strong>:</strong> Comprehensive guides and tutorials for building neural networks with Keras</li></ul><h2 id="cb9c5ac4-55f5-4f24-ae8b-ae4d5b9c6999" class=""><code>Sequential()</code> Neural Networks</h2><h3 id="0923ad08-2f13-4a62-928e-84d89f330d27" class="">Sequential Model</h3><p id="cf78dc41-3d97-4afa-99db-1837fde763cb" class="">The Sequential model in Keras is akin to a straight line of layers, perfect for neural networks that follow a direct layer-after-layer structure. Initiate by creating a Sequential model instance, then layer it with various types such as dense (fully connected), convolutional, and dropout layers tailored to your needs.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="710897fa-1f6e-4b3e-936c-f09d29fdf851" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from keras.models import Sequential
model = Sequential()
</code></pre><h3 id="73895da4-ba48-4c42-8819-96dee79adddc" class="">Adding Layers</h3><p id="09c4efe7-c173-4694-90b1-54fddcae4367" class="">To construct the network, layers are stacked in the Sequential model, with the inaugural layer needing input shape details.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="93262055-73e2-4b23-bdf8-847aafef403d" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from keras.layers import Dense, Conv2D, Flatten

# Incorporating a dense layer
model.add(Dense(units=64, activation=&#x27;relu&#x27;, input_dim=100))

# Introducing a convolutional layer
model.add(Conv2D(filters=32, kernel_size=(3, 3), activation=&#x27;relu&#x27;))

# Flattening for dense layers
model.add(Flatten())
</code></pre><h3 id="aa6cd32d-9829-4354-84bd-38aed3bd2109" class="">Compilation</h3><p id="00324af4-0218-49a3-ad79-c8ab9205b4a8" class="">Pre-training configuration is set through the <code>compile</code> method, specifying the optimizer, loss function, and metrics.</p><p id="9e968c22-31cf-416c-a029-a63380de2bae" class=""><strong>Optimizers</strong> adjust the neural network attributes, such as weights and learning rate, to minimize losses.</p><ul id="bf7b4691-6e94-4505-9e25-be1222c9f1c1" class="bulleted-list"><li style="list-style-type:disc"><strong>Adam:</strong> Known for its adaptive learning rate capabilities, making it suitable for a wide range of problems.</li></ul><ul id="b2796db3-48cf-4902-aa2d-f8e4ca54c9e6" class="bulleted-list"><li style="list-style-type:disc"><strong>SGD (Stochastic Gradient Descent):</strong> A traditional optimizer that is simple yet effective, especially with a well-tuned learning rate.</li></ul><p id="1a59d063-f444-4366-b587-2ccd83010b94" class=""><strong>Activation functions</strong> determine a node&#x27;s output based on its inputs, crucial for introducing non-linearity into the model to learn complex patterns.</p><ul id="9024a916-3a17-4cd0-bc37-d18ac5239cad" class="bulleted-list"><li style="list-style-type:disc"><strong>ReLU (Rectified Linear Unit):</strong> Commonly used for its efficiency and effectiveness.</li></ul><ul id="16f1941d-d97c-43ca-80e5-d893e0600fc3" class="bulleted-list"><li style="list-style-type:disc"><strong>Softmax:</strong> Ideal for the output layer in multi-class classification, converting logits into probabilities.</li></ul><ul id="7d3e6fc0-d6ef-45c6-8217-4595af0d2d05" class="bulleted-list"><li style="list-style-type:disc"><strong>Sigmoid:</strong> Often used in binary classification, transforming values to a range between 0 and 1.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="5b9ff2c7-77e3-4d2e-922d-1bf8b39f39fc" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">model.compile(optimizer=&#x27;adam&#x27;,
              loss=&#x27;categorical_crossentropy&#x27;,
              metrics=[&#x27;accuracy&#x27;])
</code></pre><h3 id="85ca3975-89a5-4e40-92b4-8c3fc48de544" class="">Training</h3><p id="8043bd29-bc6f-4f14-8acd-1d693789a4f9" class="">Models train on Numpy arrays of input data and labels with the <code>fit</code> method.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a16585e2-ef35-44f0-8377-1c7fe946ad9e" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all"># Given x_train and y_train as your data and labels
model.fit(x_train, y_train, epochs=10, batch_size=32)
</code></pre><h3 id="fa5a3b83-d3aa-4904-a263-0dd3feb8932f" class="">Evaluation</h3><p id="039c9951-e6ea-44b6-8473-468b364b34c9" class="">Model performance is gauged on a test dataset via the <code>evaluate</code> method.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="5cc0bb91-0290-47ca-86de-db65d6f375d0" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)
</code></pre><h3 id="3dad1300-845b-4fb2-bd0c-be51697d91fe" class="">When to Use Sequential Models:</h3><ul id="ef65696c-4821-465f-9b2e-815cdddc5e0c" class="bulleted-list"><li style="list-style-type:disc"><strong>Use Case</strong>: Ideal for scenarios with a singular input source and output destination, and when the model&#x27;s structure is a linear layer sequence.</li></ul><ul id="b6768402-a55f-41e6-9470-f356fffed724" class="bulleted-list"><li style="list-style-type:disc"><strong>Limitations</strong>: Inadequate for networks needing shared layers, multiple inputs/outputs, or branching layers. For such complexities, the Functional API is recommended.</li></ul><h2 id="7ff89cfe-7768-4ee9-a042-c4916bc4d1a0" class="">Non-Sequential and Advanced Architectures</h2><ul id="5e296407-2ee9-4ab1-870d-d0b8a86caee6" class="bulleted-list"><li style="list-style-type:disc">The Functional API, offering flexibility for intricate architectures, supports non-linear topologies, shared layers, and multi-input/output models. While potent, its complexity may be better suited for seasoned users or specific cases. For many standard tasks and beginners, the Sequential model provides a simpler yet effective approach to building neural networks.</li></ul><h2 id="434717e4-5a93-4e6f-a5d3-8372162c3ca7" class="">Common Types of Layers</h2><p id="93d870b1-ad40-45d6-8b6c-693f75d90d2e" class="">In Keras, layers are the building blocks of neural networks, and choosing the right types of layers is crucial for your model&#x27;s performance. Here are some common types of layers used in Keras:</p><p id="13528649-551a-4803-b221-f0cee84923c5" class=""><strong>Dense Layers:</strong></p><ul id="34d462f7-d9ba-4f34-956d-390c63d922fa" class="bulleted-list"><li style="list-style-type:disc">Fully connected layers where each neuron in the layer is connected to all neurons in the previous layer.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="db61fecd-3b1d-4e38-a77d-50e10dde5e1e" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">Dense(units, activation=None)
</code></pre><ul id="f0b411eb-bfcc-4e64-a3f3-3f2650149d1a" class="bulleted-list"><li style="list-style-type:disc"><code>units</code>: Number of neurons in the layer.</li></ul><ul id="00b5fd20-dd27-4300-adda-f81b0a09a9d3" class="bulleted-list"><li style="list-style-type:disc"><code>activation</code>: Activation function to use. Common choices include &#x27;relu&#x27;, &#x27;sigmoid&#x27;, and &#x27;softmax&#x27;.</li></ul><p id="1bdd2924-7eca-4753-a70d-6242f7e2937c" class=""><strong>Convolutional Layers (Conv2D):</strong></p><ul id="f1b224a2-6a35-45c4-b0da-a897c4937c46" class="bulleted-list"><li style="list-style-type:disc">Used primarily in image processing for feature extraction.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="952b4def-953f-4166-a553-ad6c2cea9d23" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">Conv2D(filters, kernel_size, strides=(1, 1), padding=&#x27;valid&#x27;, activation=None)
</code></pre><ul id="39b62cec-d3d2-46c6-9dbc-f587cf07561e" class="bulleted-list"><li style="list-style-type:disc"><code>filters</code>: Number of output filters in the convolution.</li></ul><ul id="2e5b1d24-963f-4eee-80ad-0e8f60505a38" class="bulleted-list"><li style="list-style-type:disc"><code>kernel_size</code>: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.</li></ul><ul id="2059763f-29f0-48be-8284-f83d91830132" class="bulleted-list"><li style="list-style-type:disc"><code>strides</code>: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width.</li></ul><ul id="e35e23af-11cf-4f4e-9821-a04a5679ac8b" class="bulleted-list"><li style="list-style-type:disc"><code>padding</code>: One of &#x27;valid&#x27; or &#x27;same&#x27; (case-insensitive).</li></ul><ul id="745bd644-40ef-43c5-932e-ddbbf71cc066" class="bulleted-list"><li style="list-style-type:disc"><code>activation</code>: Activation function to use.</li></ul><p id="36f2d209-2b8c-497a-bebf-37e1797d1f42" class=""><strong>Flatten Layers:</strong></p><p id="618386ae-e79f-48d3-b714-75b344084b07" class="">In neural network models, especially those dealing with image data, it is common to use convolutional layers (<code><strong>Conv2D</strong></code>) to process the image input. However, when transitioning from convolutional layers to fully connected layers (<code><strong>Dense</strong></code>), there is a need to flatten the multidimensional output of the convolutional layers into a single dimension. This is where Flatten layers come into play.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="7fb037fd-4316-4e1a-8251-5d6247fabbfb" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">model.add(Conv2D(64, (3, 3), activation=&#x27;relu&#x27;))
model.add(**Flatten()**)  # Flattens the output of the Conv2D layer
model.add(Dense(256, activation=&#x27;relu&#x27;))
</code></pre><p id="014010e4-ffb8-4f1e-baff-584dcf4fae7a" class="">In this example, the <code><strong>Flatten</strong></code> layer takes the output of the <code><strong>Conv2D</strong></code> layer, which is a 3D tensor (excluding the batch dimension), and flattens it into a 1D tensor. This flattened output then serves as the input to the subsequent <code><strong>Dense</strong></code> layer.</p><p id="5a02dc5b-3877-4be4-92d2-f0c7a0be70f0" class=""><strong>Pooling Layers (MaxPooling2D):</strong></p><ul id="679b7aff-1855-4617-b943-3cbdb546f407" class="bulleted-list"><li style="list-style-type:disc">Used to reduce the spatial dimensions (height and width) of the input volume.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="cbed6203-bbb5-4ff4-af77-f95539c4c8ca" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">MaxPooling2D(pool_size=(2, 2), strides=None, padding=&#x27;valid&#x27;)
</code></pre><ul id="d210becc-b0a1-4487-b3c3-1d0d3db0d48f" class="bulleted-list"><li style="list-style-type:disc"><code>pool_size</code>: An integer or tuple/list of 2 integers, window size over which to take the maximum.</li></ul><ul id="a07b75aa-f5f9-4bb0-999d-135e3edbcdd1" class="bulleted-list"><li style="list-style-type:disc"><code>strides</code>: An integer or tuple/list of 2 integers, specifying the strides of the pooling operation.</li></ul><ul id="1b0042d5-76bb-4e5e-8647-91e440c32172" class="bulleted-list"><li style="list-style-type:disc"><code>padding</code>: One of &#x27;valid&#x27; or &#x27;same&#x27; (case-insensitive).</li></ul><p id="4a51e8d2-11ae-4ad0-a198-6ec803b1eb8c" class=""><strong>Dropout Layers:</strong></p><ul id="a7a1b18c-8a52-404c-a0b2-a636b055100f" class="bulleted-list"><li style="list-style-type:disc">Used to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time.</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e8f989bb-6562-4c74-a7a0-3b17fa049303" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">Dropout(rate)
</code></pre><ul id="8471f0a9-4e95-4a10-b944-36e25cac2355" class="bulleted-list"><li style="list-style-type:disc"><code>rate</code>: Float between 0 and 1, fraction of the input units to drop.</li></ul><h2 id="38360c95-c9b6-4a25-ab81-1b6e686da039" class="">Recurrent Networks in Keras</h2><p id="aaff70d3-9217-4980-b98c-070d7c1d9e31" class="">Recurrent Neural Networks (RNNs) are a class of neural networks that are effective for modeling sequence data such as time series or natural language. Keras provides several layers for building RNNs, including the simple RNN layer, Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU) layers. These layers can be easily incorporated into a <code>Sequential</code> model or a more complex model architecture.</p><p id="91adb178-489c-4d6c-8f52-df9348ac64b0" class=""><strong>NOTE:</strong> RNNs can be challenging to train due to issues like vanishing and exploding gradients, and techniques like gradient clipping can be useful.</p><p id="01bf95e6-0faa-4c34-bf0e-78a8a3326b5d" class=""><strong>SimpleRNN</strong></p><ul id="5f61554d-848e-499a-a118-d6eb29443eb8" class="bulleted-list"><li style="list-style-type:disc"><strong>Description</strong>: The <code>SimpleRNN</code> layer is a fully-connected RNN where the output from the previous timestep is to be fed to the next timestep.</li></ul><ul id="e79a8994-fdbe-44b2-95c4-f9a958d203b9" class="bulleted-list"><li style="list-style-type:disc"><strong>Usage Example</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="91c70a18-6ac7-415c-8ce4-1609755075d6" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from keras.layers import SimpleRNN

model.add(SimpleRNN(units=50, activation=&#x27;tanh&#x27;, return_sequences=True))

</code></pre><ul id="000f94b7-175c-4166-8629-8bceb2153d95" class="bulleted-list"><li style="list-style-type:circle"><code>units</code>: Positive integer, dimensionality of the output space.</li></ul><ul id="a9256176-2130-49d0-8511-2e5f159d93cf" class="bulleted-list"><li style="list-style-type:circle"><code>activation</code>: Activation function to use (default is &#x27;tanh&#x27;).</li></ul><ul id="5835369e-56cd-4606-aad9-756ba1753541" class="bulleted-list"><li style="list-style-type:circle"><code>return_sequences</code>: Boolean. Whether to return the last output in the output sequence, or the full sequence.</li></ul></li></ul><p id="39bf18f9-5c43-48a2-8dd8-33a3cd1a1b03" class=""><strong>LSTM</strong></p><ul id="97b6c4bd-1b24-4e99-a82e-34e186f76a3d" class="bulleted-list"><li style="list-style-type:disc"><strong>Description</strong>: LSTM, or Long Short-Term Memory, layers are a type of RNN layer that helps avoid the vanishing gradient problem and is capable of learning long-term dependencies.</li></ul><ul id="630c5a92-870d-4c37-8ff8-4889ab451c3c" class="bulleted-list"><li style="list-style-type:disc"><strong>Usage Example</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="04184759-dd17-4ce8-8cff-da63849670d7" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from keras.layers import LSTM

model.add(LSTM(units=50, return_sequences=False))

</code></pre><ul id="7925998e-e3ae-47a0-bea2-747e36f438a6" class="bulleted-list"><li style="list-style-type:circle"><code>units</code>: Positive integer, dimensionality of the output space.</li></ul><ul id="c7c35685-6246-4674-adf6-d690f4ebdf26" class="bulleted-list"><li style="list-style-type:circle"><code>return_sequences</code>: Boolean. Whether to return the last output or the full sequence.</li></ul></li></ul><p id="da9cb46f-5c67-4a65-8c1f-ee68ae0d5da7" class=""><strong>GRU</strong></p><ul id="d217d7c2-c804-4f8d-8bbc-1c9d81c9313f" class="bulleted-list"><li style="list-style-type:disc"><strong>Description</strong>: GRU, or Gated Recurrent Unit, layers are a variant of LSTM with a simpler structure and can perform comparably to LSTM for many tasks.</li></ul><ul id="ca25bf36-85a5-4794-b058-b61f2aeebfb7" class="bulleted-list"><li style="list-style-type:disc"><strong>Usage Example</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="08324181-7115-4e18-9764-7bc48750d127" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from keras.layers import GRU

model.add(GRU(units=50, return_sequences=False))

</code></pre><ul id="9829dcf3-365c-4d78-bc00-f968906cd490" class="bulleted-list"><li style="list-style-type:circle"><code>units</code>: Positive integer, dimensionality of the output space.</li></ul><ul id="7e29398f-cd48-4ea0-8759-cd1691f17553" class="bulleted-list"><li style="list-style-type:circle"><code>return_sequences</code>: Boolean. Whether to return the last output or the full sequence.</li></ul></li></ul><h2 id="086065cf-797d-448f-b7c8-87eea76ae27e" class="">Incorporating Recurrent Layers in a Model</h2><p id="85d46fb0-c97d-4bef-884f-caca606c0ad3" class="">Recurrent layers can be used in a <code>Sequential</code> model just like any other layer. They are particularly useful for processing sequences of data, such as time series data or text. For sequence prediction tasks, it&#x27;s common to set <code>return_sequences=True</code> for all recurrent layers except the last one, allowing the model to make a prediction based on the entire input sequence.</p><h3 id="49cd5166-c62f-43b0-bcec-74e91420c9a3" class="">Example: Building a Sequential Model with Recurrent Layers</h3><p id="3a797430-c785-4399-8778-cfb1e0d07489" class="">Here&#x27;s an example of how to build a simple RNN model for sequence processing using the <code>Sequential</code> API in Keras:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="f53180bf-79c5-45ec-b5d7-855d5aec08d8" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Dense

model = Sequential()
model.add(Embedding(input_dim=10000, output_dim=32))
model.add(SimpleRNN(32, return_sequences=True))
model.add(SimpleRNN(32, return_sequences=True))
model.add(SimpleRNN(32))  # Last layer only returns the last outputs
model.add(Dense(1, activation=&#x27;sigmoid&#x27;))

model.compile(optimizer=&#x27;rmsprop&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;acc&#x27;])

</code></pre><p id="67ad7cac-e22e-4535-bf8f-906a01846f90" class="">In this example, the <code>Embedding</code> layer is used to encode input sequences into dense vectors of fixed size. The <code>SimpleRNN</code> layers process these sequences, and the <code>Dense</code> layer outputs the final prediction. This setup is typical for tasks like sentiment analysis, where the model needs to understand the entire sequence to make a prediction.</p><p id="047882ea-0a56-4e0c-a37a-92728bf23f34" class="">Recurrent layers in Keras offer a straightforward way to build powerful models for sequence data, with flexibility to fit a wide range of tasks and data types.</p><h2 id="a47834c1-7488-4529-a502-e59c90c91e16" class="">Common Methods</h2><ul id="ac42273f-5c1e-493b-9fe3-db0bb2912852" class="bulleted-list"><li style="list-style-type:disc"><code><strong>model.compile(optimizer, loss, metrics)</strong></code>:<ul id="9781df7f-2d5a-4f51-ab73-fac2cc2222fd" class="bulleted-list"><li style="list-style-type:circle">Configures the model for training.</li></ul><ul id="8d05025b-c6b4-4bf6-ba40-2f6c5aff3b86" class="bulleted-list"><li style="list-style-type:circle"><code>optimizer</code>: String (name of optimizer) or optimizer instance. Examples: &#x27;adam&#x27;, &#x27;sgd&#x27;.</li></ul><ul id="442c0e71-3585-4751-ab6a-d2722ce956ed" class="bulleted-list"><li style="list-style-type:circle"><code>loss</code>: String (name of objective function) or objective function. Examples: &#x27;categorical_crossentropy&#x27;, &#x27;mean_squared_error&#x27;.</li></ul><ul id="b00f3885-21e3-4e92-9c3c-0dd5674c02f3" class="bulleted-list"><li style="list-style-type:circle"><code>metrics</code>: List of metrics to be evaluated by the model during training and testing. Example: <code>[&#x27;accuracy&#x27;]</code>.</li></ul></li></ul><ul id="50676ce5-0117-4591-b2b6-977e776700aa" class="bulleted-list"><li style="list-style-type:disc"><code><strong>model.fit(x, y, batch_size, epochs, validation_data)</strong></code>:<ul id="182220e9-de51-4bf6-aeb9-0f8419263206" class="bulleted-list"><li style="list-style-type:circle">Trains the model for a fixed number of epochs (iterations on a dataset).</li></ul><ul id="4ec0de75-5764-4cec-893d-de0b8bb4a5ac" class="bulleted-list"><li style="list-style-type:circle"><code>x</code>: Input data.</li></ul><ul id="a4059387-8e66-41db-9e6b-c1d7dc3b315d" class="bulleted-list"><li style="list-style-type:circle"><code>y</code>: Target data.</li></ul><ul id="15b6a2a7-9d5d-43c2-b863-2857730467d8" class="bulleted-list"><li style="list-style-type:circle"><code>batch_size</code>: Number of samples per gradient update.</li></ul><ul id="878f819c-5343-406a-9da5-064a82ffadfe" class="bulleted-list"><li style="list-style-type:circle"><code>epochs</code>: Number of epochs to train the model.</li></ul><ul id="e95ae0e1-9b2f-4843-bbd2-9170b6585154" class="bulleted-list"><li style="list-style-type:circle"><code>validation_data</code>: Data on which to evaluate the loss and any model metrics at the end of each epoch.</li></ul></li></ul><ul id="0b9024e2-2d06-4dac-ba98-3876228c40b4" class="bulleted-list"><li style="list-style-type:disc"><code><strong>model.evaluate(x, y, batch_size)</strong></code>:<ul id="8a859d56-e325-4dd3-a3eb-866f4b138076" class="bulleted-list"><li style="list-style-type:circle">Returns the loss value &amp; metrics values for the model in test mode.</li></ul><ul id="fa2929ce-220f-495f-bb12-cf6a2c8e1e9c" class="bulleted-list"><li style="list-style-type:circle"><code>x</code>: Input data.</li></ul><ul id="a7f827a6-a60d-42cf-ba7f-9196f2e31ed6" class="bulleted-list"><li style="list-style-type:circle"><code>y</code>: Target data.</li></ul><ul id="9640e696-c372-43b3-b72d-c7c659183067" class="bulleted-list"><li style="list-style-type:circle"><code>batch_size</code>: Number of samples per evaluation step.</li></ul></li></ul><ul id="fed95193-2271-4e1c-9637-3620d9661bf1" class="bulleted-list"><li style="list-style-type:disc"><code><strong>model.predict(x, batch_size)</strong></code>:<ul id="c11b0c55-4939-4565-babe-0ab383b4967c" class="bulleted-list"><li style="list-style-type:circle">Generates output predictions for the input samples.</li></ul><ul id="b9e04671-2668-45b5-b651-6fc431fa8433" class="bulleted-list"><li style="list-style-type:circle"><code>x</code>: Input data.</li></ul><ul id="6e2c9aaf-c5ee-4ade-899d-fdaab2b34f73" class="bulleted-list"><li style="list-style-type:circle"><code>batch_size</code>: Number of samples per prediction step.</li></ul></li></ul><p id="2b3a0926-afb8-4ea9-bdd6-2f155522740c" class="">Understanding these layers and methods will provide a solid foundation for building a wide variety of neural network architectures in Keras.</p><h2 id="11e0b57c-1f56-4c0e-a9c4-018f3957d083" class="">Pre-trained Models</h2><p id="ccb1aa6a-2d45-41b7-b03e-2c76995033de" class="">Keras offers a suite of pre-defined and pre-trained models that can significantly accelerate the development process, especially in the domain of image recognition and classification. Among these, ResNet, VGG, and Inception stand out due to their proven effectiveness and widespread use.</p><h3 id="363f1254-ffab-412c-ab57-7e4b21f88ac4" class="">ResNet</h3><ul id="fcc8097d-fd03-4f0d-bc66-9c4ef243227d" class="bulleted-list"><li style="list-style-type:disc"><strong>Overview</strong>: ResNet, short for Residual Networks, introduces a novel architecture with &quot;skip connections&quot; or &quot;shortcut connections&quot; allowing it to effectively train very deep networks.</li></ul><ul id="75de057a-3b8c-40b0-947f-da4841031183" class="bulleted-list"><li style="list-style-type:disc"><strong>Use Cases</strong>: ResNet shines in tasks requiring deep networks, like complex image recognition and classification challenges.</li></ul><ul id="405a72aa-6c44-4340-b73d-437a6db49a02" class="bulleted-list"><li style="list-style-type:disc"><strong>Example Initialization</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="36d208cd-94d3-450b-8bbd-279afadca9ca" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from keras.applications.resnet50 import ResNet50
model = ResNet50(weights=&#x27;imagenet&#x27;)

</code></pre></li></ul><ul id="f4bf1fe1-acb4-4845-b556-cb4fc120d6de" class="bulleted-list"><li style="list-style-type:disc"><strong>Key Points</strong>:<ul id="c404db5f-44bf-4555-9460-d92717cf5500" class="bulleted-list"><li style="list-style-type:circle"><code>weights=&#x27;imagenet&#x27;</code> loads the model pre-trained on the ImageNet dataset, offering a strong feature extractor out of the box.</li></ul><ul id="f62bdd60-c506-47d5-baca-4e4ec47a3863" class="bulleted-list"><li style="list-style-type:circle">ResNet variants (e.g., ResNet50, ResNet101, ResNet152) differ in depth, allowing flexibility based on computational resources and task complexity.</li></ul></li></ul><h3 id="24bc885e-126a-4bb2-bd1d-4129a63dd871" class="">VGG</h3><ul id="f344ccf1-95dd-4874-ad54-26bfca1d92b2" class="bulleted-list"><li style="list-style-type:disc"><strong>Overview</strong>: VGG, developed by the Visual Graphics Group at Oxford, is known for its simplicity, using only 3x3 convolutional layers stacked on top of each other in increasing depth.</li></ul><ul id="dd35bbee-ca66-49ce-a8bb-328f90877bbd" class="bulleted-list"><li style="list-style-type:disc"><strong>Use Cases</strong>: Ideal for image classification and localization tasks, VGG provides excellent generalization capabilities.</li></ul><ul id="776103bf-da67-4b24-855c-1ae370c8257c" class="bulleted-list"><li style="list-style-type:disc"><strong>Example Initialization</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1e1d5627-b359-4fff-a01d-7cf946be8ab5" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from keras.applications.vgg16 import VGG16
model = VGG16(weights=&#x27;imagenet&#x27;)

</code></pre></li></ul><ul id="375e3dea-18fd-43a2-aa86-5217f2a815a4" class="bulleted-list"><li style="list-style-type:disc"><strong>Key Points</strong>:<ul id="4370875c-f8ba-4aff-84d8-9db92ab04d49" class="bulleted-list"><li style="list-style-type:circle">Similar to ResNet, <code>weights=&#x27;imagenet&#x27;</code> indicates using a pre-trained model, making VGG an effective tool for tasks with similar data distributions to ImageNet.</li></ul><ul id="407bf274-6b85-4778-9b27-ca429fb33fe2" class="bulleted-list"><li style="list-style-type:circle">VGG models (e.g., VGG16, VGG19) offer choices in depth, trading off between performance and computational efficiency.</li></ul></li></ul><h3 id="53819148-0ff2-4c99-90ef-f03bf6bb05f5" class="">Inception</h3><ul id="8b967c73-35f7-424e-99d6-9bd06121fbfc" class="bulleted-list"><li style="list-style-type:disc"><strong>Overview</strong>: The Inception architecture, starting from Inception v1 (or GoogleNet), introduces modules with parallel convolutional layers, significantly increasing the network&#x27;s width.</li></ul><ul id="cd8134af-0308-4fe2-b3c0-abb58841c89d" class="bulleted-list"><li style="list-style-type:disc"><strong>Use Cases</strong>: Inception excels in classifying and detecting objects within images, capable of focusing on local features with varying resolutions.</li></ul><ul id="3b6c3da9-b456-4067-8066-c991af5c0ad5" class="bulleted-list"><li style="list-style-type:disc"><strong>Example Initialization</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="bb6665db-cbf1-49fe-ae05-7eb8e6ece978" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">from keras.applications.inception_v3 import InceptionV3
model = InceptionV3(weights=&#x27;imagenet&#x27;)

</code></pre></li></ul><ul id="44fb81f3-21a6-4a7f-83df-7944767c7aba" class="bulleted-list"><li style="list-style-type:disc"><strong>Key Points</strong>:<ul id="31efdadb-fa6f-4267-abd9-9163ac992680" class="bulleted-list"><li style="list-style-type:circle">The use of mixed convolutional layers allows the model to capture information at various scales effectively.</li></ul><ul id="5fe598c3-415e-4eb3-880d-3966fc89ee97" class="bulleted-list"><li style="list-style-type:circle">Later versions, like InceptionV3 and Inception-ResNet, incorporate advancements from both architectures, offering improved accuracy and efficiency.</li></ul></li></ul><p id="0f093480-a012-4cbb-8d03-e08168a3448a" class="">These pre-defined models in Keras not only provide a solid foundation for numerous image processing tasks but also serve as educational tools, allowing a deeper understanding of successful neural network architectures. Leveraging these models can either be through direct application, fine-tuning for specific tasks, or as a feature extractor in a larger model pipeline.</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>